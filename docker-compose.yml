services:
  scraper-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: instagram-post-scraper
    image: instagram-post-scraper:latest
    pull_policy: build
    restart: unless-stopped
    shm_size: '2gb'
    cap_add:
      - SYS_ADMIN
    security_opt:
      - seccomp:unconfined
    init: true
    # Use host network for full internet connectivity (required for Chromium)
    network_mode: host
    # Memory limits to prevent OOM kills
    mem_limit: 4g
    mem_reservation: 2g
    # Note: ports mapping is not needed with network_mode: host
    environment:
      - NODE_ENV=production
      - PORT=3000
      - HEADLESS=true
      - DOCKER_ENV=true
      - TZ=America/Guayaquil
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./sessions:/app/sessions
    healthcheck:
      test: [ "CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
